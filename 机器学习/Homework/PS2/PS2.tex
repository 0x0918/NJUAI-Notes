% 请确保文件编码为utf-8，使用XeLaTex进行编译，或者通过overleaf进行编译

\documentclass[answers]{exam}  % 使用此行带有作答模块
% \documentclass{exam} % 使用此行只显示题目

\usepackage{xeCJK}
\usepackage{zhnumber}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{floatrow}
\usepackage{blindtext}
\pagestyle{headandfoot}
\firstpageheadrule
\firstpageheader{南京大学}{机器学习导论}{习题二}
\runningheader{南京大学}
{机器学习导论}
{习题二}
\runningheadrule
\firstpagefooter{}{第\thepage\ 页（共\numpages 页）}{}
\runningfooter{}{第\thepage\ 页（共\numpages 页）}{}


\setlength\linefillheight{.5in}

\renewcommand{\solutiontitle}{\noindent\textbf{解：}\par\noindent}

\renewcommand{\thequestion}{\zhnum{question}}
\renewcommand{\questionlabel}{\thequestion .}
\renewcommand{\thepartno}{\arabic{partno}}
\renewcommand{\partlabel}{\thepartno .}

\lstset{language=Matlab}%这条命令可以让LaTeX排版时将Matlab关键字突出显示
\lstset{
	breaklines,%这条命令可以让LaTeX自动将长的代码行换行排版
	basicstyle=\footnotesize\ttfamily, % Standardschrift
	backgroundcolor=\color[rgb]{0.95,0.95,0.95},
	keywordstyle=\color{blue},
	commentstyle=\color{cyan},
	tabsize=4,numbers=left,
	numberstyle=\tiny,
	frame=single,
	%numbers=left, % Ort der Zeilennummern
	numberstyle=\tiny, % Stil der Zeilennummern
	%stepnumber=2, % Abstand zwischen den Zeilennummern
	numbersep=5pt, % Abstand der Nummern zum Text
	tabsize=2, % Groesse von Tabs
	extendedchars=false, %
	breaklines=true, % Zeilen werden Umgebrochen
	keywordstyle=\color{red},%这一条命令可以解决代码跨页时, 章节标题, 页眉等汉字不显示的问题
	stringstyle=\color{white}\ttfamily, % Farbe der String
	showspaces=false, % Leerzeichen anzeigen ?
	showtabs=false, % Tabs anzeigen ?
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	%backgroundcolor=\color{lightgray},
	showstringspaces=false % Leerzeichen in Strings anzeigen ?
}
\renewcommand{\lstlistingname}{CODE}
\lstloadlanguages{% Check Dokumentation for further languages ...
	%[Visual]Basic
	%Pascal
	%C
	Python
	%XML
	%HTML
	%Java
}
\input{notations}

\begin{document}
\Large
\noindent
% 姓名学号
姓名：张三 \\
学号：1234567 \\
\begin{questions}
  \question [20] \textbf{没有免费的午餐定理}

  \begin{enumerate}
    \item 根据教材1.4节“没有免费的午餐”定理, 所有学习算法的期望性能都和随机胡猜一样, 是否还有必要继续进行研究机器学习算法？
    \item 教材1.4节在论述“没有免费的午餐”定理时, 默认使用了“分类错误率”作为性能度量来对分类器进行评估.若换用其他性能度量$\ell$,则教材中式（1.1）将改为
          \begin{equation}
            E_{ote}(\mathfrak{L}_a|X,f)=\sum_h \cdot\sum_{x\in\mathcal{X}-X}P(\vx)\ell(h(\vx),f(\vx))P(h|\mathcal{X},\mathfrak{L}_a)
          \end{equation}
          试证明“没有免费的午餐定理”仍成立.
  \end{enumerate}
  \begin{solution}
    % 请在此处作答
    \begin{enumerate}
      \item 依然有必要继续研究机器学习算法.

            NFL 定理有一个重要的前提, 即所有的 "问题" 出现的机会相同, 即 $f$ 是均匀分布的, 但是实际上并不是这样. 对于某一类具体的问题来说, $f$ 一般都不是均匀分布的. 因此, 面对一个具体的问题, 一般都会有一个具体的机器学习算法, 能够取得比其他机器学习算法更好的结果.

      \item 对于一个有 $k$ 种分类结果的 $k$ 分类器来说, 其真实目标函数可以是任何函数 $\mathcal{X}\mapsto \{ 1,2,\cdots ,k \}$, 函数空间为 $\{ 1,2,\cdots ,k \}^{|\mathcal{X}|}$.

            将 $f$ 视为遵循均匀分布, 则我们有预测正确的概率 $\displaystyle \operatorname{Pr}(h(\bm{x})=f(\bm{x}))=\frac{1}{k}$, 是一个常数. 预测错误的概率是 $\displaystyle \operatorname{Pr}(h(\bm{x})\neq f(\bm{x}))=\frac{k-1}{k}$.

            对于分类问题, 任意一种性能度量 $\ell(h(\bm{x}), f(\bm{x}))$ 的输出只有两种结果, 设预测正确的结果为 $c_1$, 预测错误的结果为 $c_0$, 则有分布列 $\displaystyle \operatorname{Pr}(\ell(h(\bm{x}), f(\bm{x}))=c_1) = \frac{1}{k}$, $\displaystyle \operatorname{Pr}(\ell(h(\bm{x}), f(\bm{x}))=c_0) = \frac{k-1}{k}$.

            那么有期望 $\displaystyle \mathbb{E}[\ell(h(\bm{x}), f(\bm{x}))] = \frac{1}{k}\cdot c_1+\frac{k-1}{k}\cdot c_0$, 我们可以设 $\displaystyle c=\frac{1}{k}\cdot c_1+\frac{k-1}{k}\cdot c_0$. 以二分类问题的错误率为例子, 此时 $\displaystyle c = \frac{1}{2}\cdot 0 + \frac{2-1}{2}\cdot 1 = \frac{1}{2}$.

            则我们有

            $$
              \begin{aligned}
                \sum_{f}E_{ote}(\mathfrak{L}_{a}|X,f)
                 & =\sum_{f}\sum_{h}\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\ell(h(\bm{x}), f(\bm{x}))P(h|X,\mathfrak{L}_{a}) \\
                 & =\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\sum_{h}P(h|X,\mathfrak{L}_{a})\sum_{f}\ell(h(\bm{x}), f(\bm{x})) \\
                 & =\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\sum_{h}P(h|X,\mathfrak{L}_{a})ck^{|\mathcal{X}|}                 \\
                 & =ck^{|\mathcal{X}|}\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\sum_{h}P(h|X,\mathfrak{L}_{a})                 \\
                 & =ck^{|\mathcal{X}|}\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\cdot 1                                         \\
              \end{aligned}
            $$

            可以看出, 总误差依然与学习算法无关, 因此对于其他的性能度量来说, NFL 定理依然成立.
    \end{enumerate}
  \end{solution}


  \question [15] \textbf{线性回归}

  给定包含$m$个样例的数据集$\mD=\left\{\left(\vx_{1}, y_{1}\right),\left(\vx_{2}, y_{2}\right), \cdots,\left(\vx_{m}, y_{m}\right)\right\}$, 其中$\vx_{i}=\left(x_{i 1} ; x_{i 2} ; \cdots ; x_{i d}\right) \in \mathbb{R}^{d}$, $y_{i} \in\mathbb{R}$为$\vx_{i}$的实数标记.
  针对数据集$\mD$中的$m$个示例, 教材3.2节所介绍的“线性回归”模型 要求该线性模型的预测结果和其对应的标记之间的误差之和最小:
  \begin{align}
    \left(\vw^{*}, b^{*}\right) & =\frac{1}{2}\underset{(\vw, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(\vx_{i}\right)-y_{i}\right)^{2} \notag                            \\
                                & =\frac{1}{2}\underset{(\vw, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-(\vw^\top \vx_{i}+b)\right)^{2}\;.\label{ch3_eq:linear_regression}
  \end{align}
  即寻找一组权重$(\vw, b)$, 使其对$\mD$中示例预测的整体误差最小.\footnote{公式~\ref{ch3_eq:linear_regression}中系数$\frac{1}{2}$是为了化简后续推导. 有时也会乘上$\frac{1}{m}$以计算均方误差 (Mean Square Error), 由于平均误差和误差和在优化过程中只相差一个常数, 不影响优化结果, 因此在后续讨论中省略这一系数.}
  定义$y=\left[y_{1}; \ldots, y_{m}\right] \in \mathbb{R}^{m}$, 且 $\mX=\left[\vx_{1}^\top ; \vx_{2}^\top ; \cdots ; \vx_{m}^\top\right] \in \mathbb{R}^{m \times d}$, 请将线性回归的优化过程使用矩阵进行表示.

  \begin{solution}
    %请在此处作答
  \end{solution}


  \question [25] \textbf{正则化}

  在实际问题中,我们常常会遇到示例相对较少, 而特征很多的场景. 在这类情况中如果直接求解线性回归模型, 较少的示例无法获得唯一的模型参数, 会具有多个模型能够"完美"拟合训练集中的所有样例, 实现插值 (interpolation). 此外, 模型很容易过拟合. 为缓解这些问题, 常在线性回归的闭式解中引入正则化项$\Omega(\vw)$, 通常形式如下:
  \begin{equation}
    \vw_{\mathbf{Ridge}}^{*}, b_{\mathbf{Ridge}}^{*}=\underset{\vw, b}{\arg \min }\; \frac{1}{2}\left\|\mX \vw+\ones b-\mathbf{y}\right\|_{2}^{2}+\lambda \Omega(\vw)  \label{eq:ls-regular}\;.
  \end{equation}
  其中, $\lambda > 0$为正则化参数. 正则化表示了对模型的一种偏好, 例如$\Omega(\vw)$一般对模型的复杂度进行约束, 因此相当于从多个在训练集上表现同等预测结果的模型中选出模型复杂度最低的一个.

  考虑岭回归 (ridge regression)问题, 即设置公式\eqref{eq:ls-regular}中正则项$\Omega(\vw) = \lVert \vw\rVert_2^2$. 本题中将对岭回归的闭式解以及正则化的影响进行探讨.
  \begin{enumerate}
    \item 请给出岭回归的最优解$\vw_{\mathbf{Ridge}}^*$和$b_{\mathbf{Ridge}}^*$的闭式解表达式, 并使用矩阵形式表示, 分析其最优解和原始线性回归最优解$\vw_{\mathbf{LS}}^*$和$b_{\mathbf{LS}}^*$的区别;
    \item 请证明对于任何矩阵$\mX$, 下式均成立
          \begin{align}
            \left(\mX \mX^{\top}+\lambda \mmI_m\right)^{-1} \mX=\mX\left(\mX^{\top} \mX+\lambda \mmI_d\right)^{-1}\;.
          \end{align}
          请思考, 上述的结论是否能够帮助岭回归的计算, 在何种情况下能够带来帮助?
    \item 针对波士顿房价预测数据 (\lstinline{boston}), 编程实现原始线性回归模型和岭回归模型, 基于闭式解在训练集上构建模型, 计算测试集上的均方误差 (Mean Square Error, MSE). 请参考\lstinline{LinearRegression.py}进行模型构造.
          \lstinputlisting[language=Python]{LinearRegression.py}
          \begin{enumerate}
            \item 对于线性回归模型, 请直接计算测试集上的MSE;
            \item 对于岭回归问题, 请考察不同正则项权重$\lambda$的取值范围, 并观察训练集MSE、测试集MSE和$\lambda$的取值的关系, 总结变化的规律;
          \end{enumerate} 除示例代码中使用到的sklearn库函数外, 不能使用其他的sklearn函数, 需要基于numpy实现线性回归模型和MSE的计算.
  \end{enumerate}
  \begin{solution}
    %请在此处作答
  \end{solution}

  \question [20] \textbf{线性判别分析}

  教材3.4节介绍了“线性判别分析”模型LDA (Linear Discriminative Analysis), 本题首先针对LDA从分布假设的角度进行推导和分析.
  考虑$N$分类问题, 训练集 $\left\{(\vx_1,y_1),(\vx_2,y_2),\ldots,(\vx_m,y_m)\right\}$, 其中, 第 $n$ 类样例从高斯分布 $\mathcal{N}(\vmu_n,\mSigma_n)$ 中独立同分布采样得到 (其中, $n=1,2,\cdots,N$). 记该类样例数量为 $m_n$. 类别先验为 $p\left(y=n\right)=\pi_n$, 反映了各类别出现的概率. 若$\vx\in \mathbb{R}^d \sim \mathcal{N}(\vmu,\mSigma)$, 则其概率密度函数为
  \begin{align}
    p(\vx)=\frac{1}{(2\pi)^{\frac{d}{2}}\det\left(\mSigma\right)^\frac{1}{2}}\exp\left(-\frac{1}{2}(\vx-\vmu)^\top\mSigma^{-1}(\vx-\vmu)\right).
  \end{align}
  假设不同类别的条件概率为高斯分布, 当不同类别的协方差矩阵$\mSigma_n$相同时, 对于类别的预测转化为类别中心之间的线性问题, % 因此在这一假设下, 该方法也被称为线性判别分析 , 简称LDA. 
  下面对这一模型进行进一步分析.
  假设$\mSigma_n = \mSigma$, 分析LDA的分类方式以及参数估计步骤.
  \begin{enumerate}
    \item \label{Q1q1}样例 $\vx$ 的后验概率$p\left(y=n\mid \vx\right)$表示了样例属于第$n$类的可能性, 当计算样例针对$N$个类别的后验概率后, 找出后验概率最大的类别对样例的标记进行预测, 即 $\arg\max_n$ $p\left(y=n\mid \vx\right)$. 等价于考察 $\ln p(y=n\mid \vx)$的大小,  请证明在此假设下,
          \begin{equation}
            \arg\max_y p\left(y\mid \vx\right)=\arg\max_n\; \underbrace{\vx^{\top} \mSigma^{-1} \vmu_{n}-\frac{1}{2} \vmu_{n}^{\top} \mSigma^{-1} \vmu_{n}+\ln \pi_{n}}_{\delta_n(\vx)}    \;.
          \end{equation}
          其中 $\delta_{n}(\vx)$ 为LDA在分类时的判别函数.
    \item \label{Q1q2} 在LDA模型中, 需要估计各类别的先验概率, 以及条件概率中高斯分布的参数. 针对二分类问题 ($N=2$), 使用如下方式估计类别先验、均值与协方差矩阵:
          \begin{align}
            \hat{\pi}_{n} & =\frac{m_n}{m}; \quad
            \hat{\vmu}_{n}=\frac{1}{m_n}\sum_{y_{i}=n} \vx_{i}\;,                                                                                       \\
            \hat{\mSigma} & =\frac{1}{m-N}\sum_{n=1}^{N} \sum_{y_{i}=n}\left(\vx_{i}-\hat{\vmu}_{n}\right)\left(\vx_{i}-\hat{\vmu}_{n}\right)^{\top}\;.
          \end{align}
          LDA使用这些经验量替代真实参数, 计算判别式 $\delta_n(\vx)$ 并按照第\ref{Q1q1}问中的准则做出预测. 请证明:
          \begin{equation}
            \vx^{\top} \hat{\mSigma}^{-1}\left(\hat{\vmu}_{2}-\hat{\vmu}_{1}\right)>\frac{1}{2}\left(\hat{\vmu}_{2}+\hat{\vmu}_{1}\right)^{\top} \hat{\mSigma}^{-1}\left(\hat{\vmu}_{2}-\hat{\vmu}_{1}\right)-\ln \left(m_{2} / m_{1}\right)
          \end{equation}时 LDA 将样例预测为第 2 类. 请分析这一判别方式的几何意义.
    \item 在LDA中, 对样例$\vx$的判别可视为在投影的空间中和某个阈值进行比较. 上述推导通过最大后验概率的方法得到对投影后样例分布的需求, 而Fisher判别分析 (Fisher Discriminant Analysis, FDA)也是一种常见的线性判别分析方法, 直接对样例投影后数据的分布情况进行约束.
          FDA一般通过广义瑞利商进行求解, 请基于教材3.4节对“线性判别分析”的介绍，对广义瑞利商的性质进行分析, 探讨FDA多分类推广的性质.
          下面请说明对于$N$类分类问题, FDA投影的维度最多为$N-1$, 即投影矩阵$\mW\in\mathbb{R}^{d\times (N-1)}$.\\
          提示：矩阵的秩具有如下性质：
          对于矩阵$\mathbf{A}\in\mathbb{R}^{m \times n}$, 矩阵$\mathbf{B}\in\mathbb{R}^{n \times r}$, 则
          \begin{equation}
            \operatorname{rank}(\mathbf{A})+\operatorname{rank}(\mathbf{B})-n \leq \operatorname{rank}(\mathbf{A B}) \leq \min \{\operatorname{rank}(\mathbf{A}), \operatorname{rank}(\mathbf{B})\}\;.
          \end{equation}
          对于任意矩阵$\mA$, 以下公式成立
          \begin{equation}
            \operatorname{rank}(\mA)=\operatorname{rank}\left(\mathbf{A}^{\top}\right)=\operatorname{rank}\left(\mathbf{A} \mathbf{A}^{\top}\right)=\operatorname{rank}\left(\mathbf{A}^{\top} \mathbf{A}\right)\;.
          \end{equation}
  \end{enumerate}
  \begin{solution}
    %请在此处作答
  \end{solution}


  \question [20] \textbf{多分类学习}

  教材3.5节介绍了“多分类学习”的多种方式, 本题针对OvO和OvR两种多分类学习方法进行分析:
  \begin{enumerate}
    \item 分析两种多分类方法的优劣. 思考这两种多分类推广方式是否存在难以处理的情况?
    \item 在OvR的每一个二分类子任务中, 目标类别作为正类, 而其余所有类别作为负类. 此时, 是否需要显式考虑正负类别的不平衡带来的影响?
  \end{enumerate}
  \begin{solution}
    %请在此处作答
  \end{solution}

\end{questions}


\end{document}