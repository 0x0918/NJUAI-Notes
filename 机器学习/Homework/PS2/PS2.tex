% 请确保文件编码为utf-8，使用XeLaTex进行编译，或者通过overleaf进行编译

\documentclass[answers]{exam}  % 使用此行带有作答模块
% \documentclass{exam} % 使用此行只显示题目

\usepackage{xeCJK}
\usepackage{zhnumber}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{floatrow}
\usepackage{blindtext}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\usepackage{subfigure} %插入多图时用子图显示的宏包

\pagestyle{headandfoot}
\firstpageheadrule
\firstpageheader{南京大学}{机器学习导论}{习题二}
\runningheader{南京大学}
{机器学习导论}
{习题二}
\runningheadrule
\firstpagefooter{}{第\thepage\ 页（共\numpages 页）}{}
\runningfooter{}{第\thepage\ 页（共\numpages 页）}{}


\setlength\linefillheight{.5in}

\renewcommand{\solutiontitle}{\noindent\textbf{解：}\par\noindent}

\renewcommand{\thequestion}{\zhnum{question}}
\renewcommand{\questionlabel}{\thequestion .}
\renewcommand{\thepartno}{\arabic{partno}}
\renewcommand{\partlabel}{\thepartno .}

\lstset{language=Matlab}%这条命令可以让LaTeX排版时将Matlab关键字突出显示
\lstset{
	breaklines,%这条命令可以让LaTeX自动将长的代码行换行排版
	basicstyle=\footnotesize\ttfamily, % Standardschrift
	backgroundcolor=\color[rgb]{0.95,0.95,0.95},
	keywordstyle=\color{blue},
	commentstyle=\color{cyan},
	tabsize=4,numbers=left,
	numberstyle=\tiny,
	frame=single,
	%numbers=left, % Ort der Zeilennummern
	numberstyle=\tiny, % Stil der Zeilennummern
	%stepnumber=2, % Abstand zwischen den Zeilennummern
	numbersep=5pt, % Abstand der Nummern zum Text
	tabsize=2, % Groesse von Tabs
	extendedchars=false, %
	breaklines=true, % Zeilen werden Umgebrochen
	keywordstyle=\color{red},%这一条命令可以解决代码跨页时, 章节标题, 页眉等汉字不显示的问题
	stringstyle=\color{white}\ttfamily, % Farbe der String
	showspaces=false, % Leerzeichen anzeigen ?
	showtabs=false, % Tabs anzeigen ?
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	%backgroundcolor=\color{lightgray},
	showstringspaces=false % Leerzeichen in Strings anzeigen ?
}
\renewcommand{\lstlistingname}{CODE}
\lstloadlanguages{% Check Dokumentation for further languages ...
	%[Visual]Basic
	%Pascal
	%C
	Python
	%XML
	%HTML
	%Java
}
\input{notations}

\begin{document}
\Large
\noindent
% 姓名学号
姓名：方盛俊 \\
学号：201300035 \\
\begin{questions}
  \question [20] \textbf{没有免费的午餐定理}

  \begin{enumerate}
    \item 根据教材1.4节“没有免费的午餐”定理, 所有学习算法的期望性能都和随机胡猜一样, 是否还有必要继续进行研究机器学习算法？
    \item 教材1.4节在论述“没有免费的午餐”定理时, 默认使用了“分类错误率”作为性能度量来对分类器进行评估.若换用其他性能度量$\ell$,则教材中式（1.1）将改为
          \begin{equation}
            E_{ote}(\mathfrak{L}_a|X,f)=\sum_h \cdot\sum_{x\in\mathcal{X}-X}P(\vx)\ell(h(\vx),f(\vx))P(h|\mathcal{X},\mathfrak{L}_a)
          \end{equation}
          试证明“没有免费的午餐定理”仍成立.
  \end{enumerate}
  \begin{solution}
    % 请在此处作答
    \begin{enumerate}
      \item 依然有必要继续研究机器学习算法.

            NFL 定理有一个重要的前提, 即所有的 "问题" 出现的机会相同, 即 $f$ 是均匀分布的, 但是实际上并不是这样. 对于某一类具体的问题来说, $f$ 一般都不是均匀分布的. 因此, 面对一个具体的问题, 一般都会有一个具体的机器学习算法, 能够取得比其他机器学习算法更好的结果.

      \item 对于一个有 $k$ 种分类结果的 $k$ 分类器来说, 其真实目标函数可以是任何函数 $\mathcal{X}\mapsto \{ 1,2,\cdots ,k \}$, 函数空间为 $\{ 1,2,\cdots ,k \}^{|\mathcal{X}|}$.

            将 $f$ 视为遵循均匀分布, 则我们有预测正确的概率 $\displaystyle \operatorname{Pr}(h(\bm{x})=f(\bm{x}))=\frac{1}{k}$, 是一个常数. 预测错误的概率是 $\displaystyle \operatorname{Pr}(h(\bm{x})\neq f(\bm{x}))=\frac{k-1}{k}$.

            对于分类问题, 任意一种性能度量 $\ell(h(\bm{x}), f(\bm{x}))$ 的输出只有两种结果, 设预测正确的结果为 $c_1$, 预测错误的结果为 $c_0$, 则有分布列 $\displaystyle \operatorname{Pr}(\ell(h(\bm{x}), f(\bm{x}))=c_1) = \frac{1}{k}$, $\displaystyle \operatorname{Pr}(\ell(h(\bm{x}), f(\bm{x}))=c_0) = \frac{k-1}{k}$.

            那么有期望 $\displaystyle \mathbb{E}[\ell(h(\bm{x}), f(\bm{x}))] = \frac{1}{k}\cdot c_1+\frac{k-1}{k}\cdot c_0$, 我们可以设 $\displaystyle c=\frac{1}{k}\cdot c_1+\frac{k-1}{k}\cdot c_0$. 以二分类问题的错误率为例子, 此时 $\displaystyle c = \frac{1}{2}\cdot 0 + \frac{2-1}{2}\cdot 1 = \frac{1}{2}$.

            则我们有

            $$
              \begin{aligned}
                \sum_{f}E_{ote}(\mathfrak{L}_{a}|X,f)
                 & =\sum_{f}\sum_{h}\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\ell(h(\bm{x}), f(\bm{x}))P(h|X,\mathfrak{L}_{a}) \\
                 & =\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\sum_{h}P(h|X,\mathfrak{L}_{a})\sum_{f}\ell(h(\bm{x}), f(\bm{x})) \\
                 & =\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\sum_{h}P(h|X,\mathfrak{L}_{a})ck^{|\mathcal{X}|}                 \\
                 & =ck^{|\mathcal{X}|}\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\sum_{h}P(h|X,\mathfrak{L}_{a})                 \\
                 & =ck^{|\mathcal{X}|}\sum_{\bm{x}\in \mathcal{X}-X}P(\bm{x})\cdot 1                                         \\
              \end{aligned}
            $$

            可以看出, 总误差依然与学习算法无关, 因此对于其他的性能度量来说, NFL 定理依然成立.
    \end{enumerate}
  \end{solution}


  \question [15] \textbf{线性回归}

  给定包含$m$个样例的数据集$\mD=\left\{\left(\vx_{1}, y_{1}\right),\left(\vx_{2}, y_{2}\right), \cdots,\left(\vx_{m}, y_{m}\right)\right\}$, 其中$\vx_{i}=\left(x_{i 1} ; x_{i 2} ; \cdots ; x_{i d}\right) \in \mathbb{R}^{d}$, $y_{i} \in\mathbb{R}$为$\vx_{i}$的实数标记.
  针对数据集$\mD$中的$m$个示例, 教材3.2节所介绍的“线性回归”模型 要求该线性模型的预测结果和其对应的标记之间的误差之和最小:
  \begin{align}
    \left(\vw^{*}, b^{*}\right) & =\frac{1}{2}\underset{(\vw, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(\vx_{i}\right)-y_{i}\right)^{2} \notag                            \\
                                & =\frac{1}{2}\underset{(\vw, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-(\vw^\top \vx_{i}+b)\right)^{2}\;.\label{ch3_eq:linear_regression}
  \end{align}
  即寻找一组权重$(\vw, b)$, 使其对$\mD$中示例预测的整体误差最小.\footnote{公式~\ref{ch3_eq:linear_regression}中系数$\frac{1}{2}$是为了化简后续推导. 有时也会乘上$\frac{1}{m}$以计算均方误差 (Mean Square Error), 由于平均误差和误差和在优化过程中只相差一个常数, 不影响优化结果, 因此在后续讨论中省略这一系数.}
  定义$y=\left[y_{1}; \ldots, y_{m}\right] \in \mathbb{R}^{m}$, 且 $\mX=\left[\vx_{1}^\top ; \vx_{2}^\top ; \cdots ; \vx_{m}^\top\right] \in \mathbb{R}^{m \times d}$, 请将线性回归的优化过程使用矩阵进行表示.

  \begin{solution}
    %请在此处作答
    原式可用矩阵表示为

    $$
      (\bm{w}^{*}, b^{*}) = \argmin_{(\bm{w}, b)}\frac{1}{2}(\bm{y}-\bm{X}\bm{w}-\bm{1}b)^{\top}(\bm{y}-\bm{X}\bm{w}-\bm{1}b)
    $$

    令 $\displaystyle \bm{E} = \frac{1}{2}(\bm{y}-\bm{X}\bm{w}-\bm{1}b)^{\top}(\bm{y}-\bm{X}\bm{w}-\bm{1}b)$

    对 $\bm{w}$ 求导得

    $$
      \frac{\partial \bm{E}}{\partial \bm{w}} = \bm{X}^{\top}(\bm{X}\bm{w}+\bm{1}b-\bm{y})
    $$

    对 $b$ 求导得

    $$
      \frac{\partial \bm{E}}{\partial b} = \bm{1}^{\top}(\bm{X}\bm{w}+\bm{1}b-\bm{y})
    $$

    令上面两式同时等于零即可得到 $\bm{w}$ 和 $b$ 的最优解的闭式解.

    当 $\bm{X}^{\top}\bm{X}$ 是满秩矩阵时, 则有

    $$
      \bm{w}^{*} = (\bm{X}^{\top}\bm{X})^{-1}\bm{X}^{\top}(\bm{y}-\bm{1}b^{*})
    $$

    令 $\bm{T} = (\bm{X}^{\top}\bm{X})^{-1}$, 则

    $$
      \bm{w}^{*} = \bm{T}\bm{X}^{\top}(\bm{y}-\bm{1}b^{*})
    $$

    代入有

    $$
      \begin{aligned}
        b^{*}
         & = \bm{1}^{\top}\bm{y} - \bm{1}^{\top}\bm{X}\bm{w}^{*}                                                                    \\
         & = \bm{1}^{\top}\bm{y} - \bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}(\bm{y}-\bm{1}b^{*})                                       \\
         & = \bm{1}^{\top}\bm{y} - \bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{y} + \bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{1}b^{*} \\
      \end{aligned}
    $$

    最后有

    $$
      b^{*} = \frac{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{y} - \bm{1}^{\top}\bm{y}}{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{1} - 1}
    $$

    最后将 $b^{*}$ 带回即可求出

    $$
      \bm{w}^{*} = \bm{T}\bm{X}^{\top}(\bm{y}-\bm{1}\frac{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{y} - \bm{1}^{\top}\bm{y}}{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{1} - 1})
    $$
  \end{solution}


  \question [25] \textbf{正则化}

  在实际问题中,我们常常会遇到示例相对较少, 而特征很多的场景. 在这类情况中如果直接求解线性回归模型, 较少的示例无法获得唯一的模型参数, 会具有多个模型能够"完美"拟合训练集中的所有样例, 实现插值 (interpolation). 此外, 模型很容易过拟合. 为缓解这些问题, 常在线性回归的闭式解中引入正则化项$\Omega(\vw)$, 通常形式如下:
  \begin{equation}
    \vw_{\mathbf{Ridge}}^{*}, b_{\mathbf{Ridge}}^{*}=\underset{\vw, b}{\arg \min }\; \frac{1}{2}\left\|\mX \vw+\ones b-\mathbf{y}\right\|_{2}^{2}+\lambda \Omega(\vw)  \label{eq:ls-regular}\;.
  \end{equation}
  其中, $\lambda > 0$为正则化参数. 正则化表示了对模型的一种偏好, 例如$\Omega(\vw)$一般对模型的复杂度进行约束, 因此相当于从多个在训练集上表现同等预测结果的模型中选出模型复杂度最低的一个.

  考虑岭回归 (ridge regression)问题, 即设置公式\eqref{eq:ls-regular}中正则项$\Omega(\vw) = \lVert \vw\rVert_2^2$. 本题中将对岭回归的闭式解以及正则化的影响进行探讨.
  \begin{enumerate}
    \item 请给出岭回归的最优解$\vw_{\mathbf{Ridge}}^*$和$b_{\mathbf{Ridge}}^*$的闭式解表达式, 并使用矩阵形式表示, 分析其最优解和原始线性回归最优解$\vw_{\mathbf{LS}}^*$和$b_{\mathbf{LS}}^*$的区别;
    \item 请证明对于任何矩阵$\mX$, 下式均成立
          \begin{align}
            \left(\mX \mX^{\top}+\lambda \mmI_m\right)^{-1} \mX=\mX\left(\mX^{\top} \mX+\lambda \mmI_d\right)^{-1}\;.
          \end{align}
          请思考, 上述的结论是否能够帮助岭回归的计算, 在何种情况下能够带来帮助?
    \item 针对波士顿房价预测数据 (\lstinline{boston}), 编程实现原始线性回归模型和岭回归模型, 基于闭式解在训练集上构建模型, 计算测试集上的均方误差 (Mean Square Error, MSE). 请参考\lstinline{LinearRegression.py}进行模型构造.
          \lstinputlisting[language=Python]{LinearRegression.py}
          \begin{enumerate}
            \item 对于线性回归模型, 请直接计算测试集上的MSE;
            \item 对于岭回归问题, 请考察不同正则项权重$\lambda$的取值范围, 并观察训练集MSE、测试集MSE和$\lambda$的取值的关系, 总结变化的规律;
          \end{enumerate} 除示例代码中使用到的sklearn库函数外, 不能使用其他的sklearn函数, 需要基于numpy实现线性回归模型和MSE的计算.
  \end{enumerate}
  \begin{solution}
    %请在此处作答
    \begin{enumerate}
      \item 令 $\displaystyle \bm{E} = \frac{1}{2}\left\| \bm{X}\bm{w}+\bm{1}b-\bm{y} \right\|_{2}^{2} + \lambda\left\| \bm{w} \right\|_{2}^{2}$

            对 $\bm{w}$ 求导得

            $$
              \frac{\partial \bm{E}}{\partial \bm{w}} = \bm{X}^{\top}(\bm{X}\bm{w}+\bm{1}b-\bm{y}) + 2\lambda \bm{w}
            $$

            对 $b$ 求导得

            $$
              \frac{\partial \bm{E}}{\partial b} = \bm{1}^{\top}(\bm{X}\bm{w}+\bm{1}b-\bm{y})
            $$

            当 $(\bm{X}^{\top}\bm{X}+2\lambda \bm{I}_d)$ 是满秩矩阵时, 令该式等于零即可得

            $$
              \bm{w}^{*}_{\mathbf{Ridge}} = (\bm{X}^{\top}\bm{X}+2\lambda \bm{I}_d)^{-1}\bm{X}^{\top}(\bm{y}-\bm{1}b^{*}_{\mathbf{Ridge}})
            $$

            令 $\bm{T} = (\bm{X}^{\top}\bm{X}+2\lambda \bm{I}_d)^{-1}$, 则

            $$
              \bm{w}^{*}_{\mathbf{Ridge}} = \bm{T}\bm{X}^{\top}(\bm{y}-\bm{1}b^{*}_{\mathbf{Ridge}})
            $$

            代入有

            $$
              \begin{aligned}
                b^{*}_{\mathbf{Ridge}}
                 & = \bm{1}^{\top}\bm{y} - \bm{1}^{\top}\bm{X}\bm{w}^{*}_{\mathbf{Ridge}}                                                                    \\
                 & = \bm{1}^{\top}\bm{y} - \bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}(\bm{y}-\bm{1}b^{*}_{\mathbf{Ridge}})                                       \\
                 & = \bm{1}^{\top}\bm{y} - \bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{y} + \bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{1}b^{*}_{\mathbf{Ridge}} \\
              \end{aligned}
            $$

            最后有

            $$
              b^{*}_{\mathbf{Ridge}} = \frac{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{y} - \bm{1}^{\top}\bm{y}}{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{1} - 1}
            $$

            最后将 $b^{*}_{\mathbf{Ridge}}$ 带回即可求出

            $$
              \bm{w}^{*}_{\mathbf{Ridge}} = \bm{T}\bm{X}^{\top}(\bm{y}-\bm{1}\frac{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{y} - \bm{1}^{\top}\bm{y}}{\bm{1}^{\top}\bm{X}\bm{T}\bm{X}^{\top}\bm{1} - 1})
            $$

            加上第二题的结果, 可以看出, 使用矩阵 $\bm{T}$ 抽象之后, 最优解和原始最优解的形式是一样的, 不同的是 $\bm{T}$ 的值, 二者的矩阵 $\bm{T}$ 括号内相差了一个 $2\lambda \bm{I}_d$ 项.

            所以我们可以认为 $\bm{w}^{*}_{\mathbf{LS}}$ 和 $b^{*}_{\mathbf{LS}}$ 是 $\lambda=0$ 的 $\bm{w}^{*}_{\mathbf{Ridge}}$ 和 $b^{*}_{\mathbf{Ridge}}$ 特殊情况.

      \item 将恒等式

            $$
              \bm{X} = \bm{X}
            $$

            右端乘上 $(\bm{X}^{\top}\bm{X}+\lambda I_{d})(\bm{X}^{\top}\bm{X}+\lambda I_{d})^{-1}$ 即 $I_{d}$ 则有

            $$
              \bm{X} = \bm{X}(\bm{X}^{\top}\bm{X}+\lambda I_{d})(\bm{X}^{\top}\bm{X}+\lambda I_{d})^{-1}
            $$

            将 $\bm{X}$ 乘入括号内则有

            $$
              \bm{X} = (\bm{X}\bm{X}^{\top}\bm{X}+\lambda \bm{X})(\bm{X}^{\top}\bm{X}+\lambda I_{d})^{-1}
            $$

            再将 $\bm{X}$ 提出至右侧则有

            $$
              \bm{X} = (\bm{X}\bm{X}^{\top}+\lambda I_{m})\bm{X}(\bm{X}^{\top}\bm{X}+\lambda I_{d})^{-1}
            $$

            最后则有

            $$
              (\bm{X}\bm{X}^{\top}+\lambda I_{m})^{-1}\bm{X} = \bm{X}(\bm{X}^{\top}\bm{X}+\lambda I_{d})^{-1}
            $$

            式子成立.

            这个结论能够帮助岭回归的计算.

            当样例的维度 $d$ 大于样例数目 $m$ 的时候, 将 $\bm{X}(\bm{X}^{\top}\bm{X}+2\lambda I_{d})^{-1}$ 转为 $(\bm{X}\bm{X}^{\top}+2\lambda I_{m})^{-1}\bm{X}$ 能够将矩阵求逆求 $\bm{T}$ 的矩阵维度减少, 进而加快矩阵求逆的速度.

      \item (a)

            算出线性回归模型测试集上的 MSE 结果为 20.724023437336253.

            (b)

            取不同的权重所得结果为

            \begin{table}[H]
              \centering
              \caption{lambda} \vspace{2mm}\label{table:roc}
              \begin{tabular}{c|c c c c c c c c c c c}\toprule
                \hline
                $\lambda$ & -2.0  & -1.6  & -1.2  & -0.8  & -0.4  & 0.0   & 0.4   & 0.8   & 1.2   & 1.6   & 2.0   \\ \midrule
                MSE       & 23.04 & 23.36 & 24.17 & 28.20 & 33.61 & 20.72 & 21.09 & 21.31 & 21.45 & 21.54 & 21.60 \\ \midrule
                \bottomrule
              \end{tabular}
            \end{table}

            \begin{figure}[H]
              \centering
              \includegraphics[width=0.8\textwidth]{Figure_1.png}
              \caption{P-R 曲线}
              \label{Fig.main1}
            \end{figure}


            可以看出, 当 $\lambda < 0$ 时, 最终 MSE 变化波动巨大, 很少有规律性, 所以一般不会选择; 当 $\lambda > 0$ 时, MSE 随着 $\lambda$ 的增大而缓慢增大, 所以我们可以选择一个相对较小但仍然为正数的 $\lambda$ 值.
    \end{enumerate}
  \end{solution}

  \question [20] \textbf{线性判别分析}

  教材3.4节介绍了“线性判别分析”模型LDA (Linear Discriminative Analysis), 本题首先针对LDA从分布假设的角度进行推导和分析.
  考虑$N$分类问题, 训练集 $\left\{(\vx_1,y_1),(\vx_2,y_2),\ldots,(\vx_m,y_m)\right\}$, 其中, 第 $n$ 类样例从高斯分布 $\mathcal{N}(\vmu_n,\mSigma_n)$ 中独立同分布采样得到 (其中, $n=1,2,\cdots,N$). 记该类样例数量为 $m_n$. 类别先验为 $p\left(y=n\right)=\pi_n$, 反映了各类别出现的概率. 若$\vx\in \mathbb{R}^d \sim \mathcal{N}(\vmu,\mSigma)$, 则其概率密度函数为
  \begin{align}
    p(\vx)=\frac{1}{(2\pi)^{\frac{d}{2}}\det\left(\mSigma\right)^\frac{1}{2}}\exp\left(-\frac{1}{2}(\vx-\vmu)^\top\mSigma^{-1}(\vx-\vmu)\right).
  \end{align}
  假设不同类别的条件概率为高斯分布, 当不同类别的协方差矩阵$\mSigma_n$相同时, 对于类别的预测转化为类别中心之间的线性问题, % 因此在这一假设下, 该方法也被称为线性判别分析 , 简称LDA. 
  下面对这一模型进行进一步分析.
  假设$\mSigma_n = \mSigma$, 分析LDA的分类方式以及参数估计步骤.
  \begin{enumerate}
    \item \label{Q1q1}样例 $\vx$ 的后验概率$p\left(y=n\mid \vx\right)$表示了样例属于第$n$类的可能性, 当计算样例针对$N$个类别的后验概率后, 找出后验概率最大的类别对样例的标记进行预测, 即 $\arg\max_n$ $p\left(y=n\mid \vx\right)$. 等价于考察 $\ln p(y=n\mid \vx)$的大小,  请证明在此假设下,
          \begin{equation}
            \arg\max_y p\left(y\mid \vx\right)=\arg\max_n\; \underbrace{\vx^{\top} \mSigma^{-1} \vmu_{n}-\frac{1}{2} \vmu_{n}^{\top} \mSigma^{-1} \vmu_{n}+\ln \pi_{n}}_{\delta_n(\vx)}    \;.
          \end{equation}
          其中 $\delta_{n}(\vx)$ 为LDA在分类时的判别函数.
    \item \label{Q1q2} 在LDA模型中, 需要估计各类别的先验概率, 以及条件概率中高斯分布的参数. 针对二分类问题 ($N=2$), 使用如下方式估计类别先验、均值与协方差矩阵:
          \begin{align}
            \hat{\pi}_{n} & =\frac{m_n}{m}; \quad
            \hat{\vmu}_{n}=\frac{1}{m_n}\sum_{y_{i}=n} \vx_{i}\;,                                                                                       \\
            \hat{\mSigma} & =\frac{1}{m-N}\sum_{n=1}^{N} \sum_{y_{i}=n}\left(\vx_{i}-\hat{\vmu}_{n}\right)\left(\vx_{i}-\hat{\vmu}_{n}\right)^{\top}\;.
          \end{align}
          LDA使用这些经验量替代真实参数, 计算判别式 $\delta_n(\vx)$ 并按照第\ref{Q1q1}问中的准则做出预测. 请证明:
          \begin{equation}
            \vx^{\top} \hat{\mSigma}^{-1}\left(\hat{\vmu}_{2}-\hat{\vmu}_{1}\right)>\frac{1}{2}\left(\hat{\vmu}_{2}+\hat{\vmu}_{1}\right)^{\top} \hat{\mSigma}^{-1}\left(\hat{\vmu}_{2}-\hat{\vmu}_{1}\right)-\ln \left(m_{2} / m_{1}\right)
          \end{equation}时 LDA 将样例预测为第 2 类. 请分析这一判别方式的几何意义.
    \item 在LDA中, 对样例$\vx$的判别可视为在投影的空间中和某个阈值进行比较. 上述推导通过最大后验概率的方法得到对投影后样例分布的需求, 而Fisher判别分析 (Fisher Discriminant Analysis, FDA)也是一种常见的线性判别分析方法, 直接对样例投影后数据的分布情况进行约束.
          FDA一般通过广义瑞利商进行求解, 请基于教材3.4节对“线性判别分析”的介绍，对广义瑞利商的性质进行分析, 探讨FDA多分类推广的性质.
          下面请说明对于$N$类分类问题, FDA投影的维度最多为$N-1$, 即投影矩阵$\mW\in\mathbb{R}^{d\times (N-1)}$.\\
          提示：矩阵的秩具有如下性质：
          对于矩阵$\mathbf{A}\in\mathbb{R}^{m \times n}$, 矩阵$\mathbf{B}\in\mathbb{R}^{n \times r}$, 则
          \begin{equation}
            \operatorname{rank}(\mathbf{A})+\operatorname{rank}(\mathbf{B})-n \leq \operatorname{rank}(\mathbf{A B}) \leq \min \{\operatorname{rank}(\mathbf{A}), \operatorname{rank}(\mathbf{B})\}\;.
          \end{equation}
          对于任意矩阵$\mA$, 以下公式成立
          \begin{equation}
            \operatorname{rank}(\mA)=\operatorname{rank}\left(\mathbf{A}^{\top}\right)=\operatorname{rank}\left(\mathbf{A} \mathbf{A}^{\top}\right)=\operatorname{rank}\left(\mathbf{A}^{\top} \mathbf{A}\right)\;.
          \end{equation}
  \end{enumerate}
  \begin{solution}
    %请在此处作答
    \begin{enumerate}
      \item

            由贝叶斯公式可知

            $$
              \begin{aligned}
                 & \quad\ p(y=n|\bm{x})                                                                                                                                                                                                                                           \\
                 & = \frac{p(y=n)p(\bm{x}|y=n)}{p(\bm{x})}                                                                                                                                                                                                                        \\
                 & = \frac{\displaystyle \pi_{n}((2\pi)^{\frac{d}{2}}\det (\bm{\Sigma})^{\frac{1}{2}})^{-1}\exp (-\frac{1}{2}(\bm{x}-\bm{\mu}_{n})^{\top}\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu}_{n}))}{p(\bm{x})}                                                                       \\
                 & = \frac{\displaystyle \pi_{n}((2\pi)^{\frac{d}{2}}\det (\bm{\Sigma})^{\frac{1}{2}})^{-1}\exp (\bm{x}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}-\frac{1}{2}\bm{\mu}_{n}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}-\frac{1}{2}\bm{x}^{\top}\bm{\Sigma}^{-1}\bm{x})}{p(\bm{x})} \\
              \end{aligned}
            $$

            并且 $\argmax_{n}p(y=m|\bm{x})$ 可以转化为

            $$
              \argmax_{n}\ln p(y=m|\bm{x})
            $$

            即有

            $$
              \begin{aligned}
                \argmax_{n} & \ln \pi_{n}+\ln ((2\pi)^{\frac{d}{2}}\det (\bm{\Sigma})^{\frac{1}{2}})^{-1}+\bm{x}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n} \\&-\frac{1}{2}\bm{\mu}_{n}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}-\frac{1}{2}\bm{x}^{\top}\bm{\Sigma}^{-1}\bm{x}-\ln p(\bm{x}) \\
              \end{aligned}
            $$

            去除与 $n$ 无关的项即可得

            $$
              \argmax_{n} \bm{x}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}-\frac{1}{2}\bm{\mu}_{n}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}+\ln \pi_{n}
            $$

      \item

            将式子

            $$
              \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}(\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1}) > \frac{1}{2}(\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1})^{\top}\hat{\bm{\Sigma}}^{-1}(\hat{\bm{\mu}}_{2} - \hat{\bm{\mu}}_{1}) - \ln \frac{m_{2}}{m_{1}}
            $$

            拆开可得

            $$
              \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{2} - \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{1} > \frac{1}{2}\hat{\bm{\mu}}_{2}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{2} - \frac{1}{2}\hat{\bm{\mu}}_{1}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{1}-\ln \frac{m_{2}}{m_{1}}
            $$

            进行移项

            $$
              \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{2}-\frac{1}{2}\hat{\bm{\mu}}_{2}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{2}+\ln \frac{m_{2}}{m} > \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{1}-\frac{1}{2}\hat{\bm{\mu}}_{1}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{1}+\ln \frac{m_{1}}{m}
            $$

            将 $\displaystyle \ln \frac{m_{n}}{m}$ 替换为 $\ln \hat{\pi}_{n}$ 可得

            $$
              \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{2}-\frac{1}{2}\hat{\bm{\mu}}_{2}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{2}+\ln \hat{\pi}_{2} > \bm{x}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{1}-\frac{1}{2}\hat{\bm{\mu}}_{1}^{\top}\hat{\bm{\Sigma}}^{-1}\hat{\bm{\mu}}_{1}+\ln \hat{\pi}_{1}
            $$

            即有 $\displaystyle n^{*} = \argmax_{n} \bm{x}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}-\frac{1}{2}\bm{\mu}_{n}^{\top}\bm{\Sigma}^{-1}\bm{\mu}_{n}+\ln \pi_{n} = 2$

            所以此时 LDA 将样例预测为第 2 类.

            几何意义:

            $\displaystyle \frac{1}{2}(\hat{\bm{\mu}}_{2}+\hat{\bm{\mu}}_{1})$ 是 $\hat{\bm{\mu}}_{1}$ 和 $\hat{\bm{\mu}}_{2}$ 这两个类别中心的中点.

            而 $x^{\top}\hat{\bm{\Sigma}}^{-1}(\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1})$ 可以化为 $(\hat{\bm{\Sigma}}^{-\frac{1}{2}}x)^{\top}(\hat{\bm{\Sigma}}^{-\frac{1}{2}}(\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1}))$

            即将特殊的多维正态分布缩放旋转成了标准多维正态分布, 然后衡量离第 1 类的类别中心 $\hat{\bm{\mu}}_{1}$ 的远近.

            若大于号成立, 说明 $\bm{x}$ 比中点 $\displaystyle \frac{1}{2}(\hat{\bm{\mu}}_{1}+\hat{\bm{\mu}}_{2})$ 离 $\hat{\bm{\mu}}_{1}$ 更远, 也就是离 $\hat{\bm{\mu}}_{2}$ 更近.

            但是还要考虑一个因素, 如果第 2 类的样例比第 1 类的样例数目多, 就要加入一定的偏好, 比如这里的对数几率 $\ln (m_2 / m_1)$, 当 $m_2 > m_1$ 时是正数, 就能更容易判断为第 2 类.

      \item

            求解

            $$
              \max_{\bm{W}} \frac{\operatorname{tr}(\bm{W}^{\top}\bm{S}_{b}\bm{W})}{\operatorname{tr}(\bm{W}^{\top}\bm{S}_{w}\bm{W})}
            $$

            最后可以转化为广义特征值问题求解

            $$
              \bm{S}_{b}\bm{W} = \lambda \bm{S}_{w}\bm{W}
            $$

            即求解

            $$
              \bm{S}_{w}^{-1}\bm{S}_{b}\bm{W} = \lambda \bm{W}
            $$

            也就是求 $\bm{S}_{w}^{-1}\bm{S}_{b}$ 特征值最大的前几个的特征向量.

            而我们知道

            $$
              \bm{S}_{b} = \sum_{i=1}^{N} m_{i}(\bm{\mu}_{i}-\bm{\mu})(\bm{\mu}_{i}-\bm{\mu})^{\top}
            $$

            因此我们有

            $$
              \begin{aligned}
                \operatorname{rank}(\bm{S}_{w}^{-1}\bm{S}_{b})
                 & \le \min\{ \operatorname{rank}(\bm{S}_{w}^{-1}), \operatorname{rank}(\bm{S}_{b}) \}              \\
                 & \le \operatorname{rank}(\bm{S}_{b})                                                              \\
                 & = \operatorname{rank}(\sum_{i=1}^{N} m_{i}(\bm{\mu}_{i}-\bm{\mu})(\bm{\mu}_{i}-\bm{\mu})^{\top}) \\
                 & \le \sum_{i=1}^{N} \operatorname{rank}((\bm{\mu}_{i}-\bm{\mu})(\bm{\mu}_{i}-\bm{\mu})^{\top})    \\
                 & = \sum_{i=1}^{N} \operatorname{rank}(\bm{\mu}_{i}-\bm{\mu})                                      \\
                 & = N                                                                                              \\
              \end{aligned}
            $$

            因此 $\bm{S}_{w}^{-1}\bm{S}_{b}$ 最多有 $N$ 个非零特征值, 如果选取所有的特征值, 相当于选取了所有的有效特征向量, 也就没有投影的意义了; 所以我们最多只能从 $N$ 个特征值中选择其中最大的 $N-1$ 个特征值, 对应 $N-1$ 个特征向量.

            所以 FDA 投影的维度最多为 $N-1$.
    \end{enumerate}
  \end{solution}


  \question [20] \textbf{多分类学习}

  教材3.5节介绍了“多分类学习”的多种方式, 本题针对OvO和OvR两种多分类学习方法进行分析:
  \begin{enumerate}
    \item 分析两种多分类方法的优劣. 思考这两种多分类推广方式是否存在难以处理的情况?
    \item 在OvR的每一个二分类子任务中, 目标类别作为正类, 而其余所有类别作为负类. 此时, 是否需要显式考虑正负类别的不平衡带来的影响?
  \end{enumerate}
  \begin{solution}
    %请在此处作答
    \begin{enumerate}
      \item

            OvO 的优势: OvO 的每个分类器仅用到两个类的样例, 在类别很多的时候, OvO 的训练时间开销通常比 OvR 小.

            OvO 的劣势: OvO 需要训练 $N(N-1)/2$ 个分类器, 所以存储开销和训练时间开销通常比较大.

            OvR 的优势: OvR 只需要训练 $N$ 个分类器.

            OvR 的劣势: 每次训练时都用到了所有的数据, 在类别很多的时候, 训练时间开销更大.

            这两种多分类推广方式都可能存在难以处理的情况.

            例如对 OvR 来说, 如果类别特别多的时候, 每次都要使用全部的数据来处理, 时间开销就接近 $O(n^{2})$ 了, 几乎是不可接受的.

            即使是对于 OvO 来说, 我们也能构造出一个类似的例子, 例如有一半的样例都是属于第 0 类, 但是剩下的一半几乎每个样例都对应一个类别, 这样时间开销和空间开销也都到达了 $O(n^{2})$ 了, 也几乎是不可接受的.

      \item

            对 OvR 来说, 由于对每一个类进行了相同的处理, 其拆解出来的二分类任务中类别不平衡的影响会相互抵消, 因此通常不需要显式考虑正负类别不平衡带来的影响.
    \end{enumerate}
  \end{solution}

\end{questions}


\end{document}